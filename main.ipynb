{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import openai\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from consts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('kns_csv_files/kns_committee.csv')\n",
    "df = df[df['KnessetNum'] >= 25]\n",
    "df = df[df['CategoryID'].isin([MONEY_COM_CATEGORY_ID, DEFENSE_COM_CATEGORY_ID, LAW_ORDER_COM_CATEGORY_ID, MESADERET_COM_CATEGORY_ID, KNESSET_COM_CATEGORY_ID])]\n",
    "commitee_ids = df['CommitteeID'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meeting_protocol_text(text_path):\n",
    "    # Define the URL to fetch\n",
    "    base_url = 'https://production.oknesset.org/pipelines/data/committees/meeting_protocols_text/'\n",
    "    # Send GET request to the URL\n",
    "    response = requests.get(base_url + text_path)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        response.encoding = 'utf-8'\n",
    "        # Retrieve the content of the file\n",
    "        return response.text\n",
    "    else:\n",
    "        raise ValueError(f\"Failed to retrieve content. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rate_aggressiveness' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1836\\2522381786.py\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtext_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom_session_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_parsed_filename'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_meeting_protocol_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_paths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0magg_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrate_aggressiveness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1836\\2522381786.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtext_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom_session_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_parsed_filename'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_meeting_protocol_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_paths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0magg_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrate_aggressiveness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rate_aggressiveness' is not defined"
     ]
    }
   ],
   "source": [
    "com_session_df = pd.read_csv('kns_csv_files/kns_committeesession.csv')\n",
    "com_session_df = com_session_df[com_session_df['CommitteeID'].isin(commitee_ids)]\n",
    "\n",
    "com_session_df.dropna(subset=['text_parsed_filename'], inplace=True)\n",
    "text_paths = com_session_df['text_parsed_filename'].to_list()\n",
    "texts = [get_meeting_protocol_text(path) for path in text_paths]\n",
    "agg_scores = [rate_aggressiveness(text) for text in texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knesset_members_df = pd.read_csv('kns_csv_files/kns_person.csv')\n",
    "first_names, last_names = knesset_members_df['FirstName'].to_list(), knesset_members_df['LastName'].to_list()\n",
    "knesset_members = [' '.join([first_name, last_name]) for first_name, last_name in zip(first_names, last_names)]\n",
    "\n",
    "warnings = {mem: [0, 0, 0] for mem in knesset_members}\n",
    "\n",
    "# handle members with a middle name or a nickname\n",
    "new_first_names, new_last_names = [], []\n",
    "\n",
    "for fn, ln in zip(first_names, last_names):\n",
    "    names = re.findall('\\w+', fn)\n",
    "    \n",
    "    for name in names:\n",
    "        warnings[name + ' ' + ln] = warnings[fn + ' ' + ln]\n",
    "        new_first_names.append(name)\n",
    "        new_last_names.append(ln)\n",
    "\n",
    "# update first and last names\n",
    "first_names = new_first_names\n",
    "last_names = new_last_names\n",
    "\n",
    "knesset_members = [' '.join([first_name, last_name]) for first_name, last_name in zip(first_names, last_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meeting_warnings(text, warnings, knesset_members) -> None:\n",
    "    \"\"\"\n",
    "    Return warnings from the meeting protocol text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Meeting protocol text.\n",
    "\n",
    "    warnings: Dict[str, List[int]]\n",
    "        Number of warnings for each Knesset member.\n",
    "\n",
    "    knesset_members: List[str]\n",
    "        List of Knesset members.\n",
    "    \"\"\"\n",
    "\n",
    "    # find all warnings\n",
    "    matches = re.findall(WARNING_REGEX, text, flags=re.MULTILINE)\n",
    "    print(len(matches))\n",
    "    for i, match in enumerate(matches):\n",
    "        print(f'match #{i}:')\n",
    "        print(match)\n",
    "        sentences = match.split('\\n')\n",
    "        first_sentence, last_sentence = sentences[0], sentences[-1]\n",
    "        for kns_member in knesset_members:\n",
    "            if kns_member in first_sentence:\n",
    "                word2idx = {'ראש': 0, 'שני': 1, 'שליש': 2}\n",
    "                for word, idx in word2idx.items():\n",
    "                    if word in last_sentence:\n",
    "                        warnings[kns_member][idx] += 1\n",
    "                        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_protocol_sentences(text: str) -> str:\n",
    "    ind = re.search(\"<< יור >>\", text)\n",
    "    txt2 = text[ind.span()[0]:]\n",
    "    txt2 = re.sub(\"<<.*\",\"\", txt2)\n",
    "    txt2 = re.sub(\">>.*\",\"\", txt2)\n",
    "    txt2 = re.sub(\"-\", \" \", txt2)\n",
    "    txt2 = re.sub(\"\\n\\s+\",\"\\n\", txt2)\n",
    "    txt2 = re.sub(\" +\",\" \", txt2)\n",
    "    return txt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('protocols/2159679.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "filtered_text = filter_protocol_sentences(text)\n",
    "with open('filtered_protocols/2159679.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(filtered_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlephBert Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at onlplab/alephbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['classifier.bias', 'bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "alephbert_tokenizer = AutoTokenizer.from_pretrained('onlplab/alephbert-base')\n",
    "alephbert = AutoModelForSequenceClassification.from_pretrained('onlplab/alephbert-base', num_labels=2)\n",
    "\n",
    "# Freeze the weights of the model\n",
    "for param in list(alephbert.parameters())[:-1]:\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        return text, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "agg_scores_df = pd.read_csv('agg_score_bin.csv')\n",
    "agg_scores_df.dropna(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(agg_scores_df, test_size=0.2)\n",
    "\n",
    "# Reset the index of each DataFrame\n",
    "train_set.reset_index(drop=True, inplace=True)\n",
    "test_set.reset_index(drop=True, inplace=True)\n",
    "\n",
    "agg_train = Dataset(train_set['text'], train_set['score'])\n",
    "agg_test = Dataset(test_set['text'], test_set['score'])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(agg_train, batch_size=16, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(agg_test, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:51<00:00,  4.86s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:29<00:00,  3.91s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [02:03<00:00,  5.35s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:49<00:00,  4.74s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [04:33<00:00, 11.87s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [02:25<00:00,  6.32s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [02:20<00:00,  6.12s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [02:27<00:00,  6.39s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [03:11<00:00,  8.33s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [02:34<00:00,  6.72s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [02:04<00:00,  5.42s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [02:06<00:00,  5.50s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [02:31<00:00,  6.61s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:25<00:00,  3.73s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:20<00:00,  3.48s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:31<00:00,  3.98s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:11<00:00,  3.12s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:10<00:00,  3.08s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:21<00:00,  3.53s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:25<00:00,  3.71s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:49<00:00,  4.75s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:41<00:00,  4.40s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:35<00:00,  4.14s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:29<00:00,  3.88s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:06<00:00,  2.87s/it]\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "alephbert.to(device)\n",
    "\n",
    "# Set optimizer and loss function\n",
    "optimizer = torch.optim.Adam(alephbert.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train model\n",
    "alephbert.train()\n",
    "for epoch in range(5):\n",
    "    print('epoch', epoch)\n",
    "    for texts, scores in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        scores = scores.long().to(device)\n",
    "        inputs = alephbert_tokenizer(texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        outputs = alephbert(**inputs, return_dict=False)[0]\n",
    "\n",
    "        loss = criterion(outputs, scores)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(alephbert, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total accuracy on test set: 0.5760869565217391\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "alephbert.eval()\n",
    "preds_df = pd.DataFrame(columns=['predictions', 'scores'])\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, scores in test_dataloader:\n",
    "        inputs = alephbert_tokenizer(texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        scores = scores.to(device)\n",
    "\n",
    "        outputs = alephbert(**inputs, return_dict=False)[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += scores.size(0)\n",
    "        correct += (predicted == scores).sum().item()\n",
    "\n",
    "        # outputs = outputs.reshape((16,))\n",
    "\n",
    "        # agg score is between 1 to 7\n",
    "        # outputs = np.round(outputs)\n",
    "        \n",
    "        d = {'predictions': [o.item() for o in predicted], 'scores': [s.item() for s in scores]}\n",
    "        df = pd.DataFrame.from_dict(d)\n",
    "        preds_df = pd.concat([preds_df, df], ignore_index=True, copy=False)\n",
    "\n",
    "print('total accuracy on test set:', correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>81</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        predictions  scores\n",
       "count            92      92\n",
       "unique            2       2\n",
       "top               0       0\n",
       "freq             81      49"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "116ca426afd871d658ee678036fa2f9ff9ea33b2a9b8c09422f19bc5af964702"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
